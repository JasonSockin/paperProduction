%should run with xelatex becaust the file uses pstricks



%$Id: biDir.tex,v 1.4 2000/09/05 18:41:17 m1gsa00 Exp m1gsa00 $
%\documentclass[12pt]{article}
%\pagestyle{myheadings}
%\markright{{\bf\today Draft, Comments Welcome(ganderson\char64frb.gov)}}
%\usepackage{kluwer}
%\documentclass{ifacmtg}
\documentclass{article}
\newlength{\matfieldwidth}
\usepackage{multicol}
\usepackage{fancyheadings}
\usepackage{amssymb}
%\usepackage{html}
\usepackage{moreverb}
\usepackage{theorem}
%\usepackage[dcucite]{harvard}
\usepackage[authoryear]{natbib}
%\usepackage{harvard}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amscd}

%\begin{latexonly}
%\usepackage{pstricks,pst-node,pstcol,pst-tree}
%\end{latexonly}
%\usepackage[pdf]{pstricks,pstcol,pst-node,pst-tree}
\usepackage{pstricks,pstcol,pst-node,pst-tree}
%\usepackage{auto-pst-psf}


\usepackage{program}
\normalbaroutside
%\usepackage{draftcopy}
%\usepackage{newcent}
%\usepackage{bookman}
%\usepackage{lucid}
%\usepackage{avant}
%\usepackage{authordate1-4}

\usepackage{notebook}
\textheight9.0in
\textwidth6.5in
\topmargin0.0in
\oddsidemargin0.0in


\usepackage{epsfig}
\newsavebox{\appbox}

\setcounter{MaxMatrixCols}{20}
\newcommand{ \us }{_}
\newcommand{\mH}{\mathcal{H}}
\newcommand{\mQ}{\mathcal{Q}}
\newcommand{\mZ}{\mathcal{Z}}


\newcommand{\longExpH}{\begin{bmatrix}
0&U^k_Z \mH^k_\tau&\ldots&U^k_Z \mH^k_{\theta-1}\\
U^k_N \mH^k_\tau&\multicolumn{2}{c}{\ldots}&U^k_N \mH^k_\theta
\end{bmatrix}}
\newcommand{\longExpQ}{\begin{bmatrix}\multicolumn{3}{c}{\mQ^k}\\U^k_Z \mH^k_\tau&\ldots&U^k_Z \mH^k_{\theta-1}\end{bmatrix}}

\newcommand{ \anAmp }{&}
\newcommand{ \unds }{_}
\newcommand{\matob}[2]{\mbox{$\left [  #1  \ldots  #2 \right ] $}} 

\newenvironment{prf}{
  \begin{quote}
{\bf Proof   }
}
{{\hspace*{\fill}\nolinebreak[1]\hspace*{\fill}$\blacksquare$}
  \end{quote}
}
%\newenvironment{exmpl}{}{}

\newenvironment{exmpl}{
  \color{blue}\em
{ THE EXAMPLE:   }
}
{
}


\newcommand{\boththrm}[2]{
\begin{thrm}
{#1    }
\end{thrm}
%\begin{lrbox}{\appbox}
%\begin{thrm}
%{#1    }
%\end{thrm}
%\begin{prf}
%{#2    }
%\end{prf}
%\end{lrbox}
}
\newtheorem{algrthm}{Algorithm}

{\theoremstyle{plain}\theorembodyfont{\bfseries}
\newtheorem{thrm}{Theorem}
\newtheorem{crrlry}{Corrolary}
\newtheorem{rslt}{Algorithm}
\newtheorem{pblm}{Problem Statement}
\newtheorem{dfntn}{Definition}
}
\newcommand{\hmats}{H_{-\tau}&\hspace{0.25in}&\dots&\hspace{0.25in}&H_{\theta}}
\newcommand{\hsh}{H^{\sharp\ast}_{-\tau}&\hspace{0.25in}&\dots&\hspace{0.25in}&H^{\sharp\ast}_{\theta}}
\newcommand{\hshShort}{H^{\sharp\ast}_{-\tau}&\hspace{0.25in}&\dots&\hspace{0.25in}&H^{\sharp\ast}_{\theta-1}}
\newcommand{\hsht}{
  \begin{bmatrix}
    I&0
  \end{bmatrix}
H^{\sharp\ast}_{-\tau}&\hspace{0.25in}&\dots&\hspace{0.25in}&  \begin{bmatrix}
    I&0
  \end{bmatrix}H^{\sharp\ast}_{\theta}}
\newcommand{\zsh}{Z^{\sharp\ast}_{-\tau}&\hspace{0.25in}&\dots&Z^{\sharp\ast}_{\theta-1}}
\newcommand{\hfl}{H^{\flat\ast}_{-\tau}&\hspace{0.25in}&\dots&\hspace{0.25in}&H^{\flat\ast}_{\theta}}

\definecolor{anewcolor}{rgb}{.30,.20,.60}


%\pagestyle{plain}
%\markboth{$Revision 1.8 $ -- \today \hfill }{$Revision 1.8 $ -- \today \hfill }
\begin{document}
%\epsfig{file=aGraph.ps}


%\bibliographystyle{authordate1}

\bibliographystyle{plainnat}
\setlength{\unitlength}{2em}


  
\author{Gary S. Anderson\thanks{
I wish to thank Brian Madigan, Robert Tetlow,
Jeff Fuhrer and Hoyt Bleakley for helpful comments.
I am responsible for
any remaining errors.
The views expressed herein are mine and 
do not necessarily represent the views of the Board of Governors of the Federal
Reserve System.
}}
\title{ A Reliable and Computationally Efficient Algorithm for 
Imposing the Saddle Point Property in Dynamic Models\footnote{
$Author: m1gsa00 $  $Date: 2000/09/05 18:41:17 $ $Revision: 1.4 $\newline $Source: /mq/home/m1gsa00/aim/reliable/RCS/biDir.tex,v $    
}}
%\twocolumn[
\maketitle
\begin{abstract}
\cite{ANDER:AIM1,ANDER:AIM2} describe a powerful method for solving linear saddle point models.
The algorithm has proved 
useful in a wide array of applications including
analyzing linear perfect foresight models, providing initial
solutions and asymptotic constraints for
nonlinear models.
The algorithm  solves linear problems with dozens of lags and leads
and hundreds of equations in seconds.
The technique works well for both symbolic algebra and numerical computation.

Although widely used at the Federal Reserve, few outside the central bank know
about or have used the algorithm.
This paper attempts to present the current algorithm in a more accessible
format in the hope that economists outside the Federal Reserve may also find it
useful.
In addition, over the years there have been 
 undocumented changes in approach that have improved
the efficiency and reliability
of algorithm.
This paper describes the present state of development of this set of tools.
\end{abstract}


%]
{





\pagestyle{fancy}
\lhead{\bfseries Draft: $Revision 1.8 $ -- \today  }{\bfseries Draft: $Revision 1.8 $ -- \today  }
\rhead{\bfseries\thepage}{\bfseries\thepage}

\section{Introduction and Summary}
\label{sec:intro}

George Moore and I developed this group of algorithms to facilitate research and analysis activities   at the Federal 
Reserve.\footnote{ At the Board, economists commonly refer to this family of
algorithms as the 
AIM algorithm. A metaphor relating our approach to the ``shooting method'' 
inspired the name. }
Economists at the Board have an operational need for tools
that are useful for building, estimating and simulating moderate to large
scale rational 
expectations models.
This context dictates a need for careful 
attention to computational efficiency and
numerical stability of the algorithms.

The algorithms have proved very durable and useful for staff at the central bank.
The algorithm provides helpful diagnostic information at each stage of the computation.
Figure \ref{fig:overview} presents a flow chart  summarizing the AIM algorithm.

Many economists at the federal reserve use the algorithm in their daily 
work.\cite{bomfim96,persist,realrate,stab,zerobnd,fwlb,optpol,longrate,gmmml,learn,optsac,cr96,orphanides97,andrew98,orphanides98}
Yet, few  economists outside the central bank seem to know about the method.


This paper attempts to make the method and approach more widely 
available by describing the theory alongside its implementation.
The ``C'', Matlab, Gauss and Mathematica code are available from the author at


{\small
(http://www.bog.frb.fed.us/pubs/oss/oss4/aimindex.html).
}

The most distinctive features of this approach are:
\begin{itemize}
\item its algorithm for computing the minimal dimension state space 
transition matrix(See Section \ref{sec:inessential})
\item its
use of biorthogonality to characterize the asymptotic constraints 
that guarantee stability(See Section \ref{sec:invariantSpace}).
\item the availability of a simple modelling language for characterizing
a linear model
\item the scalability 
\end{itemize}
This results in a significant
reduction in the size of the eigenvalue calculations
and an increase in reliability of the results for most problems.

Although the ideas that make the algorithm work 
are intuitive, an efficient implementation requires some
subtle features.
Experience with the algorithm has inspired some modifications, extensions and
improvements in the original algorithm described by Anderson and Moore\cite{ANDER:AIM2}. These include:
\begin{itemize}
\item Development of a symbolic algebra version of the algorithm
\item Use of QR-Decomposition for rank determination
\item Techniques for accommodating unit roots
\end{itemize}
This paper describes the new implementation of the algorithm.

Economists at the Federal Reserve 
routinely use these algorithms for solving system of equations 
with dozens of variables each with dozens of
lags and leads.
Other authors\cite{Binder95,Zadrozny96,sims96}
have contributed solutions for these types of problems.
By exploiting the structure of the problem and
doing calculations in place where possible,
 the algorithm limits the cost of adding
lags and leads to the model better than these alternative approaches.
The number of floating point operations increases with the cube of the
number of  variables in the state space. In contrast to other approaches,
the number of 
floating point operations only increases with the square of the number of 
lags and leads.



%Approaches to imposing the
%saddle point property  all contain three basic
%components.
%\begin{itemize}
%\item Characterization of linear dynamics
%\item Assessment of stability properties
%\item Characterization of constraints for asymptotic dynamics
%\end{itemize}

This paper describes how one can apply widely available 
numerical routines from LAPACK to efficiently and reliably
attack each phase of the problem.
In addition,
the example shows how symbolic algebra programs can exploit the
same solution strategy to get closed form solutions for some dynamic models.


Section \ref{sec:arzgev} describes the procedures for computing the state 
space transition matrix. 
Section \ref{sec:invariantSpace} shows how to obtain asymptotic linear
constraints and how to apply
the constraints for stability.
%Section \ref{sec:inessential} describes how to reliably streamline the computation of the stability properties
Section \ref{sec:flops} presents floating point operation counts for each
phase of the algorithm.
Section \ref{sec:exper} presents the results of numerical experiments with
the algorithm.








%\section{Saddle Point Problem Statement}

%Anderson and Moore \cite{ANDER:AIM2} outlines a procedure that computes solutions for structural models of the form
%\begin{figure*}[htbp]
%  \begin{center}
%    \leavevmode
    



%\begin{latexonly}
  

\section{The Algorithms}

\label{alg}

\begin{description}
\item[{\bf Problem Statement}]
%\begin{pblm}
\begin{gather}
\sum_{i= - \tau}^\theta { H_i  x_{ t + i } }= \Psi z_{t}, t = 0,\ldots,\infty\label{eq:canonical}\\ \intertext{with initial conditions, if any, given by constraints of the form}
x_i  =  x^{data}_i  ,  i =  - \tau, \ldots , -1 \label{eq:init}\\ \intertext{where both $\tau$ and $\theta$ are non-negative, and $x_t$ is an L dimensional vector with}
\lim_{ t \rightarrow \infty } \|x_t\|  = < \infty  \label{eq:limit}
\end{gather}
%\end{pblm}
\end{description}

The paper describes several algorithms for analyzing and solving
 various apsects of this
problem.

\begin{rslt}
Given {\em structural model matrices}, $H_i, i=-\tau,\ldots,\theta$, 
determine a matrix $Z_f$ of {\em auxiliary initial conditions } and a {\em transition matrix}
$A$ such that the $x_t$ satisfying
\begin{gather*}
Z_t \begin{bmatrix}
  x_{t-\tau}\\\vdots\\x_{t+\theta-1}
\end{bmatrix}=0\\
\begin{bmatrix}
  x_{t-\tau+1}\\\vdots\\x_{t+\theta}
\end{bmatrix}=
A 
\begin{bmatrix}
  x_{t-\tau}\\\vdots\\x_{t+\theta-1}
\end{bmatrix}
\end{gather*}  satisfy the linear homogeneous system(See Section \ref{sec:arzgev}).
\end{rslt}

\begin{rslt}
Given a transition matrix, $A$, and auxiliary initial conditions,
$Z_f$, 
determine an {\em asymptotic constraints matrix},
$Q$, such that
\begin{gather*}
Q 
\begin{bmatrix}
  x_{t-\tau}\\\vdots\\x_{t+\theta-1}
\end{bmatrix}=0 \Longrightarrow \lim_{k\rightarrow \infty} \|x_{t+k}\| < \infty
\end{gather*}
for all $x_t$ satisfying the linear homogenous system(See Section \ref{sec:invariantSpace}).
\end{rslt}



\begin{rslt}
Given an asymptotic constraints matrix, $Q$,
determine the existence and uniqueness of {\em convergent autoregression matrices}, $B_i,i=-\tau,-1$,
such that
  \begin{gather*}
 x_t=\sum_{i=-\tau}^{-1} B_i x_{t+i}
  \end{gather*}
 satisfies the linear homogeneous system and    
$\lim_{k\rightarrow \infty} \|x_{t+k}\| =<\infty$(See Section \ref{sec:invariantSpace}).
\end{rslt}

\begin{rslt}
Given structural model matrices, $H_i, i=-\tau,\ldots,\theta$ and $\Psi$,
 convergent autoregression matrices $B_i,i=-\tau,-1$
determine the existence and uniqueness of 
{\em inhomogeneous factor matrices}, $\Phi$ and $F$ such that
\begin{gather*}
 x_t=\sum_{i=-\tau}^{-1} B_i x_{t+i} + 
  \begin{bmatrix}
  0&\dots 0&I
  \end{bmatrix}
\sum_{s=0}^\infty ( F^{s} 
\begin{bmatrix}
0\\
\Phi \Psi z_{t+s}  
\end{bmatrix})
\end{gather*}
satisfies the linear inhomogeneous system(See Section \ref{sec:inhomog}).
\end{rslt}

\begin{rslt}
Given structural model matrices, $H_i, i=-\tau,\ldots,\theta$ and $\Psi$,
 convergent autoregression matrices $B_i,i=-\tau,-1$,
 expectations for the exogenous
factors,
$E[z_{t+s}|I_t],s=0,\ldots,\infty$, convergent autoregression matrices, 
$B_i, i=-1\ldots,-\tau$ and inhomogeneous factor matrices,
$\Upsilon$ and $\Phi$
and setting 
\begin{equation*}
  \label{eq:exp}
\begin{split}E[x_{t+k}|I_t]=&\\
& \sum_{i=-\tau}^{-1} B_i E[x_{t+k+i}|I_t] \\+ 
&  \begin{bmatrix}
  0&\dots 0&I
  \end{bmatrix}
\sum_{s=0}^\infty ( F^{s} 
\begin{bmatrix}
0\\
\Phi \Psi  E[z_{t+k+s}|I_t]
\end{bmatrix}),\\&k=0,\ldots,\infty
\end{split}
\end{equation*}
determine the existence and uniqueness of an
{\em observable structure matrix},$S$, and {\em stochastic transition
matrices} $\varphi$, $\varrho$ such that
\begin{gather*}
\epsilon_t= S \begin{bmatrix}
 x_{t-\tau}\\\vdots\\x_{t}
\end{bmatrix}\\
\begin{bmatrix}
 x_{t-\tau+1}\\\vdots\\x_{t}
\end{bmatrix}= 
\varrho
\begin{bmatrix}
 x_{t-\tau}\\\vdots\\x_{t-1}
\end{bmatrix} +
\varphi
\begin{bmatrix}\epsilon_{t} +
%\Psi (E[z_t|I_t]-E[z_t|I_{t-1}])
\Psi E[z_t|I_t]
\end{bmatrix}
\end{gather*}
(See Section \ref{sec:applications}).
\end{rslt}

%    \caption{Summary of Algorithms}
%    \label{algSum}
%  \end{center}
%\end{figure*}


%Our task will be to determine whether
%The model \ref{eq:canonical} has a unique solution, an infinity of
% solutions or no solutions at all.



%The specification \ref{eq:canonical} is not restrictive.
%One can handle inhomogeneous version  of equation \ref{eq:canonical}
%by recasting the problem in terms of  deviations from a steady state value or
%by adding a new variable for each non-zero right hand side with an equation
%guaranteeing the  value always equals  
%the inhomogeneous value($x^{con}_t =x^{con}_{t-1}$ and $x^{con}_{t-1} = x^{RHS}$).

Saddle point problems combine initial conditions and asymptotic 
convergence to identify their solutions.
The uniqueness of solutions to 
system  \ref{eq:canonical} requires that
the transition matrix characterizing the linear system have an appropriate
number of explosive and stable eigenvalues\cite{blanchard80},
and that the asymptotic linear constraints 
are linearly independent of explicit and implicit initial 
conditions\cite{ANDER:AIM2}.

The solution methodology entails 
\begin{enumerate}
\item using equation \ref{eq:canonical} to
compute a state space transition matrix.
\item Computing the eigenvalues and the invariant space associated with
large eigenvalues
\item Combining the constraints provided by:
  \begin{enumerate}
  \item the
initial conditions,
\item  auxiliary initial conditions identified in the computation of the transition matrix and 
\item the invariant space vectors
  \end{enumerate}
\end{enumerate}

Figure \ref{fig:overview} presents a flow chart of the summarizing the
algorithm. For a description of a parallel implementation see \cite{ANDER:PARA}
For a description of a continuous time application see \cite{anderson97}.

\begin{figure}[htbp]
  \begin{center}
    \leavevmode
\fbox{    
    \begin{minipage}{0.45\textwidth}
\begin{picture}(10,20)
\input{ aimComponentsOverall.tex}
\end{picture}
     \end{minipage}
}   \caption{Algorithm Overview}
    \label{fig:overview}
  \end{center}
\end{figure}

%Anderson and Moore \cite{ANDER:AIM2} demonstrates that 



%Given the coefficient matrix
%\begin{gather*}
%\matob{H_{-\tau}}{H_\theta}
%\end{gather*}
%the procedure computes the reduced form coefficient matrix
%\begin{gather*}
%\matob{B_{-\tau}}{ B_{-1}}
%\end{gather*}
%for any model satisfying assumptions \ref{asmone} and \ref{asmtwo}.  If the model does not satisfy assumptions \ref{asmone} and \ref{asmtwo}, the procedure indicates whether there are no convergent solutions or a multiplicity of convergent solutions.

  
\begin{exmpl}
  

The paper describe how to 
apply the algorithm to the following model.\cite{sims96}

\begin{gather*}
  w_t=\frac{1}{N}E_t[\sum_{i=0}^{n-1}W_{t+i}] - \alpha(u_t-u_n) + \nu_t\\
W_t=\frac{1}{N}\sum_{i=0}^{n-1}w_{t-i} \\
u_t=\theta_{t-1}+\gamma W_t +\mu+\epsilon_t\\
E_t[\nu_{t+i}]=E_t[\epsilon_{t+i}]=0 \,\forall i\ge0
\end{gather*}
In Section \ref{alg}, $N=2$. Section \ref{sec:numericalExp}, explores the relationship
between $N$ and computation time.


\end{exmpl}

\subsection{Algorithm \ref{alg:unconstrainedAR}  State Space Transition Matrix Computation}
\label{sec:arzgev}




The first phase of the algorithm
applies full rank linear transformations 
to the original linear system to express
$x_{t+\theta}$ in in terms of $x_{t-\tau} \ldots x_{t+\theta-1}$.
This algorithm generalizes and implements a procedure known as the shuffling algorithm.\cite{luenberger78}
It produces an autoregressive representation for the evolution of the components of
the state space vectors along with a set of vectors 
that provide implicit initial conditions. Section \ref{sec:inessential} will explain the importance
of the latter set of vectors.

Given a system 
\begin{gather}
  \begin{bmatrix}
    \hsh
  \end{bmatrix} 
  \begin{bmatrix}
    x_{-\tau}\\ \vdots \\x_{\theta}
  \end{bmatrix} =0\nonumber \\ \intertext{ with $H^{\sharp\ast}_{\theta}$ non singular. Let} \label{eq:easy}
\Gamma^\sharp=- (H^{\sharp\ast}_{\theta})^{-1}\begin{bmatrix}
    \hshShort
  \end{bmatrix} \\ \intertext{ Then }
x_{\theta} = 
 \Gamma^\sharp
  \begin{bmatrix}
    x_{-\tau}\\ \vdots \\x_{\theta-1}
  \end{bmatrix}  \nonumber
\end{gather}

This unconstrained auto-regression in $x_t$ provides exactly what one needs to
construct the state space transition matrix. 



\begin{gather*}
A^\sharp= 
\begin{bmatrix}
  \begin{matrix}
    0&I
  \end{matrix}\\ \Gamma^\sharp
\end{bmatrix}\\ \intertext{ so that }
  \begin{bmatrix}
    x_{-\tau+1}\\ \vdots \\x_{\theta}
  \end{bmatrix}  = A
  \begin{bmatrix}
    x_{-\tau}\\ \vdots \\x_{\theta-1}
  \end{bmatrix} 
\end{gather*}

Algorithm \ref{alg:unconstrainedAR} transforms an equation system like equation \ref{eq:canonical} into an equivalent system of the form given in Equation \ref{eq:easy}. The process of transformation
generates auxiliary initial conditions that the dynamic system must obey
in addition to the explicit initial conditions in equation system (\ref{eq:canonical}-\ref{eq:limit}).


Figure \ref{fig:unconstrained} presents  a flow chart for 
an algorithm for computing the 
components of the state space transition matrix and the auxiliary initial
conditions(shift right equations).

\begin{figure*}[htbp]
  \begin{center}
    \leavevmode
\fbox{
\begin{picture}(20,28)
  \input{ unconstrainedAR.tex}
\end{picture}
}
    \caption{Algorithm to Compute Unconstrained Auto-regression: Algorithm \ref{alg:unconstrainedAR}:$\mathcal{F}_1(\mathcal{H})$}
    \label{fig:unconstrained}
  \end{center}
\end{figure*}

Figure \ref{tableau} presents a graphic characterization of the relevant
sets of linear constraints.
\begin{figure*}[htbp]
  \begin{center}
    \leavevmode
    
\fbox{
\begin{pspicture}(15,10)
\psframe[fillstyle=solid,fillcolor=gray](0,10)(6,9)
\psframe[fillstyle=solid,fillcolor=gray](4,6)(10,5)
\psframe[fillstyle=solid,fillcolor=gray](5,5)(11,4)
\psframe[fillstyle=solid,fillcolor=gray](9,0)(15,1)
\psline[linestyle=dashed]{<->}(4,8.5)(6,6.5)
\psline[linestyle=dashed]{<->}(9,3.5)(11,1.5)
\psline[linewidth=0.6mm](0,5)(15,5)
\psline[linewidth=0.6mm](5,0)(5,10)
\psline[linewidth=0.6mm](10,0)(10,10)
\psset{arrows=->}
\rput(7.5,8.0){\rnode{A}{$\tau+\theta$}}
\rput(5,8.0){\rnode{B}{}}
\rput(10,8.0){\rnode{C}{}}
\ncline{A}{B}
\ncline{A}{C}
\rput(10.5,5.0){\rnode{D}{}}
\rput(12.5,7.5){\rnode{E}{$H^{\sharp,0}$}}
\nccurve[angleB=90,angleA=270]{E}{D}
\end{pspicture}
}

    \caption{Matrix Tableau Characterization of Algorithm: Initial Tableau}
    \label{tableau}
  \end{center}
\end{figure*}
In the figure the regions where the coefficients are potentially  non-zero are shaded gray. If $H^{\sharp,0}_\theta$ is singular, one can find a linear 
combination of the rows which  preserves the rank of $H^{\sharp,0}_\theta$ but
which annihilates one or more of its rows.  Consequently, one can pre-multiply
the matrices presented in Figure \ref{tableau} to get the distribution of
zeros displayed in Figure \ref{annihilation}.
\begin{figure*}[htbp]
  \begin{center}
    \leavevmode
    
\fbox{
\begin{pspicture}(15,10)
\psframe[fillstyle=solid,fillcolor=gray](0,10)(6,9)
\psframe[fillstyle=solid,fillcolor=gray](4,6)(10,5)
\psframe[fillstyle=solid,fillcolor=gray](5,5)(11,4)
\psframe[fillstyle=solid,fillcolor=gray](9,0)(15,1)
\psline[linestyle=dashed]{<->}(4,8.5)(6,6.5)
\psline[linestyle=dashed]{<->}(10,2.5)(11,1.5)
\psline[linewidth=0.6mm](0,5)(15,5)
\psline[linewidth=0.6mm](5,0)(5,10)
\psline[linewidth=0.6mm](10,0)(10,10)
\psline(10,4.5)(11,4.5)
\psline(10,4.0)(11,4.0)
\psframe[fillstyle=solid,fillcolor=white](10,4.5)(11,5)
\psframe[fillstyle=solid,fillcolor=gray](6,4)(12,3)
\psframe[linestyle=dashed,linewidth=0.7mm](5,3.5)(11,4.5)
\psline(11,3.5)(12,3.5)
\psline(11,3.0)(11,4.0)
\psframe[fillstyle=solid,fillcolor=white](11,3.5)(12,4)
%\psframe[linestyle=dashed,fillstyle=crosshatch](6,2.5)(12,3.5)
\psline(14,0.5)(15,0.5)
\psline(14,0.0)(14,1.0)
\psframe[fillstyle=solid,fillcolor=white](14,0.5)(15,1)
\psset{arrows=->}
\rput(10.5,4.5){\rnode{D}{}}
\rput(12.5,7.5){\rnode{E}{$H^{\sharp,1}$}}
\nccurve[angleB=90,angleA=270]{E}{D}
\rput(5.0,4.75){\rnode{F}{}}
\rput(2.5,2.5){\rnode{G}{$Z^{\sharp,1}=F^{\sharp,1}$}}
\nccurve[angleB=180]{G}{F}
\end{pspicture}
}

    \caption{Matrix Tableau Characterization of Algorithm: Forward Row Annihilation}
    \label{annihilation}
  \end{center}
\end{figure*}

One can  regroup the rows in the new tableau to get $H^{\sharp,1}$.
By construction, the rank of $H^{\sharp,1}_\theta$ can be no less than
the rank of $H^{\sharp,0}_\theta$.
One can prove that repeating 
this process of annihilating and regrouping rows ultimately  produces 
an $H^{\sharp,k}=H^{\sharp,\ast}$ with $H^{\sharp,\ast}_\theta$ non-singular.
  

\NumberProgramstrue
\sfvariables
\begin{algrthm}
\label{alg:unconstrainedAR}
\begin{program}
\mbox{Given $H$,}
\mbox{ compute the unconstrained autoregression.} 
\FUNCT \mathcal{F}_{\ref{alg:unconstrainedAR}}(H) \BODY
k:=0
\mathcal{Z}^0:=\varnothing
\mathcal{H}^0:=H
\Gamma=\varnothing
\WHILE \mathcal{H}^k_\theta \text{ is singular } \cap rows(\mathcal{Z}^k) < L(\tau+\theta) 
\DO
U^k=\begin{bmatrix}U^k_Z\\U^k_N\end{bmatrix}:=|rowAnnihilator|(\mH^k_\theta)
\mH^{k+1}:= \longExpH
\mZ^{k+1}:= \longExpQ
k:=k+1
\OD
\Gamma=- H_{\theta}^{-1}\begin{bmatrix} H_{-\tau}&\hspace{0.25in}&\dots&\hspace{0.25in}&H_{\theta-1}  \end{bmatrix}
A= \begin{bmatrix}  \begin{matrix}    0&I  \end{matrix}\\ \Gamma\end{bmatrix}
|return| \{ \begin{bmatrix}\mH^k_-\tau&\ldots&\mH^k_{\theta}\end{bmatrix},A,\mZ^k \}
\ENDFUNCT
\end{program}
\end{algrthm}
{\color{anewcolor}
\begin{thrm}
Let 
{\small
\begin{gather*} \mathcal{H}=\left .
  \begin{bmatrix}
\hmats\\
&\hmats\\
&&\ddots\\
&&&\hmats\\
  \end{bmatrix} \right \} \text{${\scriptstyle\tau+\theta+1}$} 
\end{gather*}}
There are two cases:
\begin{itemize}
\item When $\mathcal{H}$ is full rank the algorithm terminates with 
 $Z^{\sharp\ast}$($Z^{\flat\ast}$) and non-singular
 $H^{\sharp\ast}_{\theta}$($H^{\flat\ast}_{\tau}$)
\item When $\mathcal{H}$ is not full rank the algorithm terminates when
some row of $
\begin{bmatrix}
\mathcal{H}^k_{-\tau}\ldots\mathcal{H}^k_\theta 
\end{bmatrix}$ is zero.
\end{itemize}

\end{thrm}
}
\begin{prf}
Consider the case when $\mathcal{H}$ is full rank.
  Each step of the algorithm applies a  rank preserving
pre-multiplication by a nonsingular matrix. Each step of the algorithm
where $H^{\sharp,k}_\theta$ is singular, increases the row rank of
$Z^{\sharp,k}$ by one.  At each step $\mathcal{Z}^{\sharp,k}$ are full rank. The rank of $Z^{\sharp,k}$ cannot exceed $L(\tau + \theta)$.
\end{prf}
  Annihilating and regrouping rows ultimately produces $H^{\sharp,\ast}$.
Figure \ref{fullRankFor} displays a potential terminal 
tableau for the example model. There are many potential choices for
annihilating the rows of the $H$ matrices, but they all produce linear systems
imposing equivalent constraints on the trajectories of the model variables.


The following corollary indicates that models 
with unique steady-states always terminate with non singular $\mathcal{H}^{\sharp,\ast}_{\theta}$.

\begin{crrlry}
If $ ( {\sum_{i= - \tau}^\theta { H_i}} )$ is non singular 
then
\begin{enumerate} 
\item $\mathcal{H}$ is full rank.
\item The origin is the unique steady state of equation\ref{eq:canonical}.
\item there exists a sequence of elementary row
operations that transforms $\mathcal{H}$ into $\mathcal{H}^\ast$
\end{enumerate} 
\end{crrlry}
\begin{prf}
  Suppose
$\mathcal{H}$ 
 Then there is
a non zero vector $V \ni V \mathcal{H}  =0$. Consequently,
\begin{gather*}
  \begin{bmatrix}
    V_{-\tau}\ldots V_{\theta}
  \end{bmatrix}
 \mathcal{H}  
 \begin{bmatrix}
   I&\ldots&I\\
\vdots&\ldots&\vdots\\
I&\ldots&I&\\
 \end{bmatrix}=0\\ \intertext{ and }
V_i ({\sum_{j= - \tau}^\theta { H_j}})=0 \, \forall i
\end{gather*}
So that $({\sum_{i= - \tau}^\theta { H_i}})$ must be singular.
\end{prf}


%Section \ref{sec:invariantSpace} 
%explains the importance of recording and employing the 
%auxiliary initial conditions(shift right equations) of $Z^{\sharp\ast}$.

%One could develop specific 
%properties of the tableau
%to characterize a canonical form for the tableau,
%instead,  
This paper presents
algorithms for efficiently obtaining the normal form characterized
in Figure \ref{fullRankBoth}.
\begin{figure*}[htbp]
  \begin{center}
    \leavevmode
    
\fbox{
\begin{pspicture}(15,10)
\psframe[fillstyle=solid,fillcolor=gray](0,10)(6,9)%upper left
\psframe[fillstyle=solid,fillcolor=gray](4,6)(10,5)%mid back

\psline[linestyle=dashed]{<->}(4,8.5)(6,6.5)%upper diag
\psframe[fillstyle=solid,fillcolor=gray](5,5)(10,2.5)%zapped

\psline[linewidth=0.6mm](0,5)(15,5)%vertical left
\psline[linewidth=0.6mm](5,0)(5,10)%midline
\psline[linewidth=0.6mm](10,0)(10,10)%vertical right

\psframe[fillstyle=solid,fillcolor=gray](5,3.0)(11,2.0)%mid front
\psline(10,2.5)(11,2.5)
\psline(10,2.0)(11,2.0)
\psframe[fillstyle=solid,fillcolor=white](10,2.5)(11,3.0)
\psframe[fillstyle=solid,fillcolor=gray](6,2.0)(12,1.0)
\psframe[linestyle=dashed,linewidth=0.7mm](5,1.5)(11,2.5)
\psframe[fillstyle=solid,fillcolor=black](10,1.5)(11,2.5)

%\psline(11,3.5)(12,3.5)
%\psline(11,3.0)(11,4.0)
\psframe[fillstyle=solid,fillcolor=white](11,1.5)(12,2.0)
%\psframe[linestyle=dashed,fillstyle=crosshatch](6,2.5)(12,3.5)

\psset{arrows=->}
\rput(10.5,2.5){\rnode{D}{}}
\rput(12.5,7.5){\rnode{E}{$H^{\sharp,k}$ with $H^{\sharp,k}_\theta$ non-singular}}
\nccurve[angleB=90,angleA=270]{E}{D}
\rput(5.0,4.0){\rnode{F}{}}
\rput(2.5,4.5){\rnode{G}{$Z^{\sharp,k}$}}
\nccurve[angleB=180]{G}{F}
\rput(5.0,2.75){\rnode{H}{}}
\rput(2.5,1.5){\rnode{G}{$F^{\sharp,k}$}}
\nccurve[angleB=180]{G}{H}

\end{pspicture}
}

    \caption{Matrix Tableau Characterization of Algorithm: Full Rank Leading Block }
    \label{fullRankFor}
  \end{center}
\end{figure*}

  

\begin{exmpl}
  

\input{biDirFigH0.tex}
\end{exmpl}
One can adapt the algorithm to compute 
\begin{gather*}
x_{-\tau} =\Gamma^\flat  
\begin{bmatrix}
  x_{\theta}\\ \vdots x_{-\tau +1}
\end{bmatrix}
\end{gather*}
Section \ref{sec:inessential} explains the utility of exploiting the
auxiliary initial conditions(shift left equations) of $Z^{\flat\ast}$.
%Subsequent sections of this paper explain the importance of retaining and using
%the other components of the tableau.


Where with $(H^{\flat,\ast}_\theta)^{-1} $ non-singular:
\begin{gather*}
  \Gamma^\flat  = (H^{\flat,\ast}_\theta)^{-1} 
\begin{bmatrix}
  H^{\flat,\ast}_{-\tau} &\ldots  & H^{\flat,\ast}_{\theta-1}
\end{bmatrix}
\end{gather*}

It is possible to express $x_{\tau}$ in terms of leads of the $x$'s. Figure \ref{fullRankBoth} provides
a graphical representation of the normal form associated with computing
these  expressions.
The State Space Reduction(see Section \ref{sec:inessential}) step of the algorithm exploits the constraints generated by 
constructing the non singular block in both the forward and backward direction.







\subsection{Invariant Space Calculations}
\label{sec:invariantSpace}
In order to compute solutions to equation \ref{eq:canonical} that converge, 
one must rule out explosive trajectories. Blanchard and Kahn\cite{blanchard80} 
used eigenvalue and eigenvector calculations to characterize the space in 
which the solutions must lie. The AIM algorithm exploits biorthogonality and
the computation of left invariant space vectors to constrain
the trajectories so that the solution cannot  explode.


The
asymptotic analysis exploits the fact that
each left  eigenvectors is orthogonal to each right eigenvector associated 
with roots of different value.
Since vectors in the left invariant space
associated with roots outside the unit circle are orthogonal to right eigenvectors associated with roots 
inside the
unit circle, a given state vector that is part of a convergent trajectory
must be orthogonal to each of these left invariant space vectors.
\begin{thrm}
  Consider a left invariant space and a right invariant space
with no eigenvalues in common. Suppose
$V_1$ spans the left invariant space and $W_2$ spans the right invariant space.
  \begin{gather*}
V_1 A = T_1 V_1\\
A W_2=  W_2 T_2
  \end{gather*}
With eigenvalues of $T_1 \neq T_2$.
Then  $V_1 W_2=0$
\end{thrm}
\begin{prf}
A right eigenvector $x_i$ and a left-eigenvector $y_j$ corresponding to distinct
eigenvalues $\lambda_i$ and $\lambda_j$ are orthogonal.\cite{NOBLE}
The left invariant space vectors are linear combination of the
left eigenvectors and the right invariant space vectors are a linear combination
of the right eigenvectors of the transition matrix raised to some finite power.
\begin{gather*}
  V_1 = \beta_{1} 
  \begin{bmatrix}
y_1\\ \vdots\\ y_J
  \end{bmatrix}\\
  W_2 = 
  \begin{bmatrix}
x_1& \dots &x_K
  \end{bmatrix}
\alpha_{2}\\
V_1 W_2 =  \beta_{1} 
  \begin{bmatrix}
y_1\\ \vdots \\ y_J
  \end{bmatrix}
  \begin{bmatrix}
x_1& \dots &x_K
  \end{bmatrix}
\alpha_{2} = 0
\end{gather*}
\end{prf}

\begin{thrm}
Let $\{x^{conv}_t\}$, $t= -\tau,\ldots,\infty$ be a non explosive solution satisfying
equation \ref{eq:canonical}. Let $A$ be the state space transition matrix
for equation
\ref{eq:canonical} and $V$ be a set of
invariant space vectors spanning the invariant space
associated with roots of
$A$ of magnitude bigger than $1$. Then for $t= 0,\ldots,\infty$
\begin{gather*}
V 
\begin{bmatrix}
  x^{conv}_{t-\tau}\\
\vdots\\
  x^{conv}_{t+\theta-1}
\end{bmatrix}=0
\end{gather*}
\end{thrm}
\begin{prf}
Using  $W$, the left generalized eigenvectors of $A$, one can employ the
 Jordan Canonical Form of A to write
\begin{gather*}
  W^H A  = J W^H \\
\intertext{ so that}
A^t = (W^H)^{-1} J^t W^H\\
y_t = A^t y_0\\
W^H y_t = J^t W^H y_0\\
\lim_{t\rightarrow \infty} y_t =0 \Rightarrow
\lim_{t\rightarrow \infty} W^H y_t =0 \\\intertext{Consequently,}
W_i^H y_0 =0 \, \forall i \ni |J_{ii}| > 1.\\ \intertext{ so that }
V y_0 = \alpha W^H y_0 = 0
\end{gather*}
\end{prf}

Thus,  if $A$ has roots with magnitude $1$ then a path converges to 
a limit cycle(or fixed point)  if and only if
\begin{gather*}
V 
\begin{bmatrix}
  x_{t-\tau}\\
\vdots\\
  x_{t+\theta-1}
\end{bmatrix}=0
\end{gather*}
for some $t$.


\begin{crrlry}
Let $\{x_t\}$, $t= -\tau,\ldots,\infty$ be a  solution satisfying
equation \ref{eq:canonical}.  If $A$ has no roots with magnitude $1$ then the 
path converges to the
unique steady state if and only if
\begin{gather*}
V 
\begin{bmatrix}
  x_{t-\tau}\\
\vdots\\
  x_{t+\theta-1}
\end{bmatrix}=0
\end{gather*}
for some $t$.
\end{crrlry}
\begin{prf}
  \begin{gather*}
W_i^H y_0 =0 \, \forall i \ni |J_{ii}| > 1.\\ \intertext{ means $y_t$ }
    J_{ii} \ne 1.\\
y_t = A^t y_0
  \end{gather*}
\end{prf}
All convergent solutions converge to the same fixed point.



%One can define equivalence classes of paths indexed by  components of the path
%since  may converge to different phases of
%the limit cycle.


Combining V and $Z^\sharp$  completely characterizes the space of 
stable solutions satisfying the linear system.
\begin{thrm}
  
Let
\begin{gather*}
  Q= 
  \begin{bmatrix}
    Z^{\sharp}\\V
  \end{bmatrix}
= 
  \begin{bmatrix}
    Q_L&Q_R
  \end{bmatrix}
\end{gather*}
The existence of convergent solutions depends on the magnitude of the
rank of the augmented
matrix 
\begin{gather*}
r_1=rank \left (  
\begin{bmatrix}
    I&0&x_{data}\\
Q_L&Q_R&0
  \end{bmatrix} 
\right ) 
\\ 
\intertext{ and  }
r_2=rank \left (  \begin{bmatrix}
    I&0\\
Q_L&Q_R
  \end{bmatrix} \right )
\end{gather*}
and  $L(\tau+\theta)$, the number of unknowns.


\begin{enumerate}
\item If $r_1 > r_2$ there is no nontrivial convergent solution
\item If $r_1 = r_2 = L(\tau + \theta)$ there is a unique convergent solution
\item If $r_1 = r_2 < L(\tau + \theta)$ the system has an infinity of convergent 
solutions
\end{enumerate}

\end{thrm}
\begin{prf}
The theorem applies well known 
results on existence and uniqueness of solutions to 
linear equation systems\cite{NOBLE}.
If
$\mathcal{M}_2=\begin{bmatrix}
  x_{data}\\0
\end{bmatrix}$ 
does not lie in the column space of $\mathcal{M}_1=\begin{bmatrix}
    I&0\\
Q_L&Q_R
  \end{bmatrix}$, then there is no solution.
If $\mathcal{M}_2$ lies in the column space of  $ \mathcal{M}_1 $ and the latter matrix is full rank, then there is a unique
solution.
If $\mathcal{M}_2$ lies in the column space of  $ \mathcal{M}_1$ and the latter matrix is not full rank, then there are
multiple solutions.
\end{prf}

The first set of equations come from the explicit initial conditions.
The second set of equations come from the equations in equation system 
\ref{eq:canonical} which do not appear in the transformed system of Equation \ref{eq:easy}
but must nevertheless be satisfied. The last set of equations come from the
constraint that the solutions are non-explosive.
There are three cases.


\begin{crrlry}
  When $Q$ has $L \theta$ rows, $Q_R$ is square.
If  $Q_R$ is non-singular, the system has a unique solution and
\begin{gather*}
    \begin{bmatrix}
    B\\B_2\\ \vdots \\ B_{\theta}  
  \end{bmatrix}
= Q_R^{-1} Q_L
\end{gather*}
  If $Q_R$ is singular, the system has an infinity of solutions.
\end{crrlry}
\begin{crrlry}
  When $Q$ has fewer than $L \theta$ rows,
The system has an infinity of solutions.
\end{crrlry}
\begin{crrlry}
  When $Q$ has more than $L \theta$ rows,
The system has a unique nontrivial 
solution only for specific values of $x_{data}$
\end{crrlry}
\begin{algrthm}
\label{alg:asymptoticConstraints}
\begin{program}
\mbox{Given $V,Z^{\sharp,\ast}$,}
\FUNCT \mathcal{F}_{\ref{alg:asymptoticConstraints}}(A,Z^{\sharp,\ast})
\text{Compute $V$, the vectors spanning the left}
\text{ invariant space associated with eigenvalues }
\text{ greater than one in magnitude}
Q:=\begin{bmatrix}Z^{\sharp,\ast}\\V\end{bmatrix}
|return|Q
\ENDFUNCT
\end{program}
\end{algrthm}
\begin{algrthm}
\label{alg:bmat}
\begin{program}
\mbox{Given $Q$,}
\FUNCT \mathcal{F}_{\ref{alg:bmat}}(Q)
|cnt|=noRows(Q)
|return|\begin{cases}
\{Q,\infty\} &|cnt| < L\theta 
\{Q,0\} &|cnt| > L\theta 
\{Q,\infty\}&(Q_R singular) 
\{B=-Q_R^{-1} Q_L,1\} &otherwise
\end{cases}
\ENDFUNCT
\end{program}
\end{algrthm}

  

\begin{exmpl}
\input{simsQB.tex}
\end{exmpl}


The formulae in Table \ref{golub} indicates that the 
number of floating operations depends on the size of  $A$, the dimension
of the dominant invariant space, $k-1$, and the relative sizes of the roots.
Convergence is slower and the number of floating point operations larger
if the magnitude of roots just inside the dominant space is close to the 
magnitude of roots just outside the dominant space.

For a given saddle point problem,  the problem
specification dictates the dimension of the dominant space 
of interest, and the model parameters determine the root configuration.
One can, however, streamline the computations by computing a similar matrix
and carrying out the eigenspace calculations on a sub-matrix of the similar
matrix and reconstructing the invariant space vectors for the original
matrix.
The algorithms in Section \ref{sec:inessential} show how to construct this
similarity transformation and how to recover the invariant space vectors for
the original problem.










\section{Inhomogeneous Solution}
\label{sec:inhomog}


\begin{gather}
\sum_{i= - \tau}^\theta { H_i  x_{ t + i } }= 0, t \geq 0 \label{eq:canonical}\\ \intertext{ with initial conditions, if any, given by constraints of the form}
x_t  =  x^{data}_t  ,  t =  - \tau, \ldots , -1\\ \intertext{where both $\tau$ and $\theta$ are non-negative, and $x_t$ is an L dimensional vector with}
\lim_{ t \rightarrow \infty } x_t  =  0
\end{gather}

Modelers can augment the homogeneous linear perfect foresight solutions with
particular solutions characterizing stochastic elements or exogenous factors.
Suppose
\begin{gather*}
\sum_{i= - \tau}^\theta { H_i  x_{ t + i } }= \Psi z_{t}, t \geq 0 \label{eq:noncanonical}\\
\lim_{ t \rightarrow \infty } x_t  =  x^\ast\\
\lim_{ t \rightarrow \infty } z_t  =  z^\ast \\ \intertext{with }
\sum_{i= - \tau}^\theta { H_i  x^\ast}= \Psi z^\ast
\end{gather*}



The $Q$ matrix plays a fundamental role in all subsequent calculations.
This set of linear restrictions codifies all the constraints needed to
guarantee the existence and uniqueness of saddle point solutions. These
solutions provide a foundation for characterizing many other useful 
model solutions and properties.


Then one can use $Q$ to write solutions in the form
\begin{equation*}
  \begin{split}
(x_{t}-x^\ast) = &\\
&\begin{bmatrix}
B_L& B_R  
\end{bmatrix}
\begin{bmatrix}
    (x_{t-\tau}-x^\ast)\\ \vdots\\(x_{t-1}-x^\ast)
  \end{bmatrix} + \\
&  \begin{bmatrix}
  0&\dots 0&I
  \end{bmatrix}
\sum_{s=0}^\infty ( F^{s} 
\begin{bmatrix}
0\\
\Phi \Psi z_{t+s}  
\end{bmatrix})\label{basic}
  \end{split}
\end{equation*}

Where
\begin{gather*}
B=  \begin{bmatrix}
B_L&B_R\\
\vdots&\vdots\\
B_L^\theta&B_R^\theta
  \end{bmatrix}
= Q_R^{-1} Q_L
\end{gather*}


\begin{gather*}
  \Phi= (H_0 + H_+  \begin{bmatrix}
    B_R\\\vdots\\B^\theta_R
  \end{bmatrix})^{-1}\\
F=
\begin{bmatrix}
0&I\\
\vdots&&\ddots\\
0&&&I\\
-\Phi H_+\begin{bmatrix}
0\\ \vdots \\ 0\\I
  \end{bmatrix}&
-\Phi H_+\begin{bmatrix}
0\\ \vdots\\I\\B_R
  \end{bmatrix}&\ldots&
-\Phi H_+\begin{bmatrix}
I\\B_R\\\vdots\\B^{\theta-1}_R
  \end{bmatrix}
\end{bmatrix}
\end{gather*}

When 
\begin{gather*}
  z_{t+1} = \Upsilon z_t\\ \intertext{the infinite sum simplifies to give}
(x_{t}-x^\ast) = \begin{bmatrix}
B_L& B_R  
\end{bmatrix}
%  \begin{bmatrix}
%     (x_{t-\tau}-x^\ast)\\\vdots\\(x_{t-1} -x^\ast)
%   \end{bmatrix} +\sum_{s=0}^\infty ( F^s \Phi \Psi\Upsilon^{s} )z_{t}\\
% (x_{t}-x^\ast) = \begin{bmatrix}
% B_L& B_R  
% \end{bmatrix}
 \begin{bmatrix}
    (x_{t-\tau}-x^\ast)\\\vdots\\(x_{t-1}-x^\ast)
  \end{bmatrix} + \vartheta z_t
\\
\intertext{ where }
vec\vartheta =   \begin{bmatrix}
  0&\dots 0&I
  \end{bmatrix}
(I - \Upsilon^T\otimes F)^{-1} vec
\begin{bmatrix}
0\\ \vdots \\ 0 \\
\Phi\Psi
\end{bmatrix}
\end{gather*}


\begin{gather*}
  \tilde{B}=
  \begin{bmatrix}
    \begin{matrix}
0&I\\      
    \end{matrix}\\
B
  \end{bmatrix}\\
B^{k+1} = B^k \tilde{B}
\end{gather*}




\begin{gather*}
  \begin{bmatrix}
    H_{-}&H_{0}&H_{+}
  \end{bmatrix}
  \begin{bmatrix}
    I\\
\begin{matrix}
      B\\\vdots\\B^{\theta+1}
    \end{matrix}
  \end{bmatrix}=0
\end{gather*}

\begin{gather*}
  \begin{bmatrix}
    H_{-}&H_{0}&H_{+}
  \end{bmatrix}
  \begin{bmatrix}
    I\\
\begin{bmatrix}
  \begin{matrix}
0&I    
  \end{matrix}\\
      B\\\vdots\\B^\theta
    \end{bmatrix}\tilde{B}
  \end{bmatrix}=0
\end{gather*}



\begin{gather*}
  H_{-} + 
(  \begin{bmatrix}
    0&H_0
  \end{bmatrix} + H_+ 
  \begin{bmatrix}
    B\\\vdots\\B^\theta
  \end{bmatrix})\tilde{B} = 0\\
  H_{-} + 
(  \begin{bmatrix}
    0&H_0
  \end{bmatrix} + H_+ 
  \begin{bmatrix}
    B_L&B_R\\\vdots&\vdots\\B^\theta_L&B^\theta_R
  \end{bmatrix})\tilde{B} = 0
\end{gather*}
\begin{gather*}
    \begin{bmatrix}
    B_L&B_R\\\vdots&\vdots\\B^\theta_L&B^\theta_R
  \end{bmatrix}\tilde{B} = 
  \begin{bmatrix}
    0&B_L\\\vdots&\vdots\\0&B^\theta_L
  \end{bmatrix}+
  \begin{bmatrix}
    B_R\\\vdots\\B^\theta_R
  \end{bmatrix}B
\end{gather*}

\begin{gather*}
  H_- + H_+  \begin{bmatrix}
    0&B_L\\\vdots&\vdots\\0&B^\theta_L
  \end{bmatrix}    + (H_0 + H_+  \begin{bmatrix}
    B_R\\\vdots\\B^\theta_R
  \end{bmatrix})B=0
\end{gather*}

\begin{gather*}
  \Phi= (H_0 + H_+  \begin{bmatrix}
    B_R\\\vdots\\B^\theta_R
  \end{bmatrix})^{-1}\\
\Phi H_- + \Phi H_+  \begin{bmatrix}
    0&B_L\\\vdots&\vdots\\0&B^\theta_L
  \end{bmatrix}  +B = 0
\end{gather*}


\begin{gather*}
  \begin{bmatrix}
    H_{-}&H_{0}&H_{+}
  \end{bmatrix}
  \begin{bmatrix}
    I&&&0\\&\ddots&&\vdots&\\&&I&0\\
0&\dots&0&I\\
0&&\begin{matrix}
      B\\\vdots\\B^\theta
    \end{matrix}
  \end{bmatrix}
  \begin{bmatrix}
    x_{t+s-\tau}\\\vdots\\x_{t+s}
  \end{bmatrix}= \Psi z_{t+s}
\end{gather*}

%\begin{gather*}
  \begin{multline*}
\Phi\Psi z_{t+s}=\\
\Phi  (\begin{bmatrix}
H_-&0
  \end{bmatrix} + 
  \begin{bmatrix}
    0&H_0
  \end{bmatrix} + \\
  \begin{bmatrix}
    H_+ \begin{bmatrix}
    0&B_L\\\vdots&\vdots\\0&B^\theta_L
  \end{bmatrix}& H_+\begin{bmatrix}
    B_R\\\vdots\\B^\theta_R
  \end{bmatrix}
  \end{bmatrix})  \begin{bmatrix}
    x_{t+s-\tau}\\\vdots\\x_{t+s}
  \end{bmatrix}
  \end{multline*}
%  \end{gather*}

  \begin{gather*}
  \begin{bmatrix}
    -B&I
  \end{bmatrix}
\begin{bmatrix}
    x_{t+s-\tau}\\\vdots\\x_{t+s}
  \end{bmatrix}= \Phi\Psi z_{t+s}\\
x_{t+s} = B \begin{bmatrix}
    x_{t+s-\tau}\\\vdots\\x_{t+s-1} 
  \end{bmatrix}+\Phi\Psi z_{t+s}
\end{gather*}


\begin{multline*}
\Phi  (\begin{bmatrix}
H_-&0
  \end{bmatrix} + 
  \begin{bmatrix}
    0&H_0
  \end{bmatrix} + \\
  \begin{bmatrix}
    H_+ \begin{bmatrix}
    0&B_L\\\vdots&\vdots\\0&B^\theta_L
  \end{bmatrix}& H_+\begin{bmatrix}
    B_R\\\vdots\\B^\theta_R
  \end{bmatrix}
  \end{bmatrix})  \begin{bmatrix}
    x_{t+s-\tau-1}\\\vdots\\x_{t+s-1}
  \end{bmatrix}+ \\ \Phi H_+\begin{bmatrix}
I\\B_R\\\vdots\\B^{\theta-1}_R
  \end{bmatrix} \Phi \Psi z_{t+s}=0
\end{multline*}

\begin{gather*}
    \begin{bmatrix}
    -B&I
  \end{bmatrix}
\begin{bmatrix}
    x_{t+s-\tau-1}\\\vdots\\x_{t+s-1}
  \end{bmatrix}+  \Phi H_+\begin{bmatrix}
I\\B_R\\\vdots\\B^{\theta-1}_R
  \end{bmatrix} \Phi\Psi z_{t+s}=0\\
x_{t+s-1} = B \begin{bmatrix}
    x_{t+s-\tau-1}\\\vdots\\x_{t+s-2} 
  \end{bmatrix}  -\Phi H_+\begin{bmatrix}
I\\B_R\\\vdots\\B^{\theta-1}_R
  \end{bmatrix} \Phi\Psi z_{t+s}\\
x_{t+s-i} = B \begin{bmatrix}
    x_{t+s-\tau-1}\\\vdots\\x_{t+s-2} 
  \end{bmatrix} + (-\Phi H_+\begin{bmatrix}
I\\B_R\\\vdots\\B^{\theta-1}_R
  \end{bmatrix})^{i} \Phi\Psi z_{t+s}
\end{gather*}

And by superposition
\begin{gather*}
x_{t} = B \begin{bmatrix}
    x_{t-\tau}\\\vdots\\x_{t-1} 
  \end{bmatrix} +\sum_{s=0}^\infty ( (-\Phi H_+\begin{bmatrix}
I\\B_R\\\vdots\\B^{\theta-1}_R
  \end{bmatrix})^{s} \Phi\Psi z_{t+s})
\end{gather*}

For constants(ie. inhomogeneous equation) require
\begin{gather*}
(\sum_{i=-\tau}^\theta H_i) x^\ast =0\\
  (x_t-x^\ast)= B(x_{t-1}-x^\ast) + \sum_{s=0}^\infty F^i c\\
\vartheta=( I - I\otimes F) vec(\phi)\\
(I - B) x^\ast = \vartheta c
\end{gather*}

\begin{algrthm}
\label{alg:inhomog}
\begin{program}
\mbox{Given $H,\Psi$,}
\FUNCT \mathcal{F}_{\ref{alg:inhomog}}(H,\Psi,B)
\Phi=(H_0 + H_+  \begin{bmatrix}B_R\\\vdots\\B^\theta_R \end{bmatrix})^{-1}
F=\begin{bmatrix}
0&I\\\vdots&&\ddots\\0&&&I\\-\Phi H_+\begin{bmatrix}0\\ \vdots \\ 0\\I  \end{bmatrix}&-\Phi H_+\begin{bmatrix}0\\ \vdots\\I\\B_R  \end{bmatrix}&\ldots&-\Phi H_+\begin{bmatrix}I\\B_R\\\vdots\\B^{\theta-1}_R  \end{bmatrix}\end{bmatrix}
|return|(\Phi,F)
\ENDFUNCT
\end{program}
\end{algrthm}

\section{Observable Structure}
\label{sec:applications}


\subsection{``Aligned Information Set'' Expectation Error Calculations}
\label{sec:errorCalc}

These saddle point calculations often arise in rational expectations models.
To compute the error term for estimation of the coefficients of these models,
one must commit to a particular information set.
Two typical choices are t and t-1 period expectations.


\begin{multline*}
  \epsilon_t =
  \begin{bmatrix}
    H_{-\tau}\ldots H_0
  \end{bmatrix} 
  \begin{bmatrix}
    x^{data}_{t-\tau}\\
\vdots \\ 
   x^{data}_{t}
  \end{bmatrix} +\\
  \begin{bmatrix}
    H_{1}\ldots H_\theta
  \end{bmatrix} 
  \begin{bmatrix}
    E [x_{t+1} |I_t] \\
\vdots \\
    E [x_{t+\theta}|I_t]
  \end{bmatrix} - \Psi z_t
\end{multline*}

Suppose 
$\exists K^\ast \ni x_{t-k} \in I_t, \forall k \ge k^\ast\\$ then set,
\begin{gather*}
    \begin{bmatrix}
    E [x_{t+1} |I_t] \\
\vdots \\
    E [x_{t+\theta}|I_t]
  \end{bmatrix}  =
\begin{bmatrix}
      B\\\vdots\\B^\theta
    \end{bmatrix}
 \tilde{B}^{k^\ast}
  \begin{bmatrix}
    x^{data}_{t-\tau+1-k^\ast}\\
\vdots \\ 
   x^{data}_{t-k^\ast}
  \end{bmatrix} +\\
  \begin{bmatrix}
\sum_{s=0}^\infty ( (-\Phi H_+\begin{bmatrix}
I\\B_R\\\vdots\\B^{\theta-1}_R
  \end{bmatrix})^{s} \Phi\Psi z_{t+s+1})\\
  \vdots\\
\sum_{s=0}^\infty ( (-\Phi H_+\begin{bmatrix}
I\\B_R\\\vdots\\B^{\theta-1}_R
  \end{bmatrix})^{s} \Phi\Psi z_{t+s+\theta})
  \end{bmatrix}
\end{gather*}

\begin{gather*}
  \epsilon_t= S   \begin{bmatrix}
    x^{data}_{t-\tau+1-\max(1,k^\ast)}\\
\vdots \\ 
   x^{data}_{t}
  \end{bmatrix} - \Psi z_t\\ \intertext{ where }
S=   \begin{bmatrix}
0_{L\times L\max(0,k^\ast-1)}&    H_{-\tau}&\ldots&H_0
  \end{bmatrix} +\\
  \begin{bmatrix}
  \begin{bmatrix}
    H_{1}\ldots H_\theta
  \end{bmatrix} 
\begin{bmatrix}
      B\\\vdots\\B^\theta
    \end{bmatrix}
 \tilde{B}^{k^\ast}
   & 0_{L\times L\max(0,k^\ast-1)}
  \end{bmatrix}
\end{gather*}

Note that for $k^\ast \ge 1$
\begin{gather*}
  \frac{\partial \epsilon_t}{\partial x^{data}_t} = H_0
\end{gather*}
and for $k^\ast = 0$
\begin{gather*}
  \frac{\partial \epsilon_t}{\partial x^{data}_t} = H_0 +   \begin{bmatrix}
    H_{1}\ldots H_\theta
  \end{bmatrix} \begin{bmatrix}
      B\\\vdots\\B^\theta
    \end{bmatrix} = \Phi^{-1}
\end{gather*}

\begin{gather*}
  \varphi = 
  \begin{cases}
    \phi & k^\ast = 0\\
    H_0^{-1} & k^\ast > 0
  \end{cases}
\varrho=
\end{gather*}

One can write
\begin{gather*}
  \begin{bmatrix}
    x_{t-\tau-\max (k^\ast-1,0)+1}\\ \vdots \\ x_{t}
  \end{bmatrix}=
\mathcal{A}   \begin{bmatrix}
    x_{t-\tau-\max (k^\ast-1,0)}\\ \vdots \\ x_{t-1}
  \end{bmatrix}  + 
\mathcal{B} \epsilon_t\intertext{ where }
\mathcal{A}=
\begin{bmatrix}
  0 &I&&\\
\vdots&&\ddots&\\
0&&&I\\
S_0^{-1} S_{t-\tau-\max (k^\ast-1,0)+1} &\dots &\dots&S_0^{-1} S_{-1} 
\end{bmatrix}\\
\mathcal{B}=
\begin{bmatrix}
  0\\\vdots \\ 0 \\ S_0^{-1} 
\end{bmatrix}
\end{gather*}

\begin{algrthm}
\label{alg:obs}
\begin{program}
\mbox{Given $H,\Psi$,B}
\FUNCT \mathcal{F}_{\ref{alg:obs}}(H,\Psi,B)
|return|(\varrho,\varphi)
\ENDFUNCT
\end{program}
\end{algrthm}



\section{Unit Roots or Constant Growth}



%\begin{thrm}
%If there exist S $\ni$
%\begin{gather*}
%S A S^{-1} = 
%\begin{bmatrix}
%  A_u&0&0\\
%\Upsilon_{um}&A_m&0\\
%\Upsilon_{ul}&\Upsilon_{ml}&A_l
%\end{bmatrix}\\ \intertext{ then one can compute eigenvalues and eigenvectors in matrices of smaller dimension. If }
%x A_u = M_u x\\
%y A_m = M_m y\\
%z A_l = M_l z\\
%S 
%\begin{bmatrix}
%  x & 0&0
%\end{bmatrix} A = M_u S 
%\begin{bmatrix}
%  x&0&0
%\end{bmatrix}\\
%S 
%\begin{bmatrix}
%  x_y &y&0
%\end{bmatrix} A = M_m S 
%\begin{bmatrix}
%  x_y&y&0
%\end{bmatrix}\\ \intertext{ with }
%vec(x_y)=- ((A^T_u \otimes I) - (M_m \otimes I))^{-1}vec(y \Upsilon_{um})\\
%S 
%\begin{bmatrix}
%  x_z &y_z&z
%\end{bmatrix} A = M_l S 
%\begin{bmatrix}
%  x_z&y_z&z
%\end{bmatrix}\\ \intertext{ with }
%vec(y_z)=- ((A^T_m \otimes I) - (M_m \otimes I))^{-1}vec(z \Upsilon_{ml})\\
%vec(x_z)=- ((A^T_u \otimes I) - (M_l \otimes I))^{-1}vec(y \Upsilon_{ul})
%\end{gather*}

%\end{thrm}
%\begin{prf}
%  Under construction.
%\end{prf}



%\begin{dfntn}
%Linked Autonomous
%Any system that can be written using Permutations of the rows and columns
%\begin{gather*}
% \bar{ \Gamma} = P^T \Gamma (I \otimes P) \\ \intertext{ with }
%\Gamma_i=  \begin{bmatrix}
%\Gamma_{u,-i}&0&0\\
%\eta_{um,-i}&\Gamma_{m,-i}&0\\
%\eta_{ul,-i}&\eta_{ml,-i}&\Gamma_{m,-i}
%  \end{bmatrix}
%\end{gather*}
%\end{dfntn}


%Consider the autonomous system 
%\begin{gather}
%  \begin{bmatrix}
%    v_{1,t}\\v_{2,t}\\u_t
%  \end{bmatrix}= 
%  \begin{bmatrix}
%    \exp^{i 2 \pi/k+\phi}&0&0\\
%0&    \exp^{-i 2 \pi/k+\phi}&0\\
%    \frac{\exp^{i 2 \pi/k+\phi}}{2}&\frac{\exp^{-i 2 \pi/k+\phi}}{2}&0\\
%  \end{bmatrix}
%  \begin{bmatrix}
%    v_{1,t-1}\\v_{2,t-1}\\u_{t-1}
%  \end{bmatrix}\label{uroot}
%\end{gather}


%Adding $  \alpha \frac{v_{1,t} + v_{2,t}}{2}$  to any equation does not 
%change any eigenvalues.
%\subsection{Unit Roots}

%\begin{thrm}
%We can solve models with unit roots by computing the solution with no unit
%roots and then inferring the result of including a unit root.
%\end{thrm}
%\begin{prf}
%  Under construction.
%\end{prf}
%\begin{algrthm}
%\label{alg:unitRoot}
%\begin{program}
%\mbox{Given $h,H$,}
%\mbox{asymptotic stability constraints}
%\FUNCT \mathcal{F}_{\ref{alg:unitRoot}}(V,Z^{\sharp,\ast})
%\ENDFUNCT
%\end{program}
%\end{algrthm}

%\begin{exmpl}
  

%\input{simsUnit.tex}
%\end{exmpl}

%\subsection{Constant Growth}

\begin{thrm}
Suppose we have for some scalar valued variable $y_t$,
\begin{gather*}
\sum_{i=-\tau}^\theta H_i x_{t+i} - G y_t\\
y_t= g y_{t-1} \\ \intertext{ then  solving the system }
\sum_{i=-\tau}^\theta g^i H_i \bar{x}_{t+i} \\ \intertext{determines the evolution
of $x_t$ }
\bar{x}^\ast = (\sum_{i=-\tau}^\theta g^i H_i)^{-1} G\\
Q (\bar{x}_t - \bar{x}_t^\ast) = 0\\
x_{t+k} = g^{k+1} y_{t-1} \bar{x}_{t+k}
\end{gather*}
\end{thrm}

The scalar g may be bigger than one in magnitude.


\section{Implementation}
The following sections describe the symbolic algebra and numeric algebra
implementations of the routines. The design of the numeric algebra routines 
must take care due to the computation with finite precision arithmetic.
These routines rely on unitary matrices to limit rounding error.
The symbolic algebra routines can often use infinite precision arithemetic
and often work faster avoiding routines that renormalize linear equations.


\subsection{State Space Reduction}



\label{sec:inessential}

One can use vectors associated with zero eigenvalue and 
the vectors associated with unit roots to reduce the size of the
eigenspace calculation.
State space reduction renders the transition matrix full rank
and  also typically reduces
the dimension by 2/3. Consequently, the computational 
burden reduces to 1/9 that of the original problem.
Thus, given $A,Z$ one can obtain eigenvalues and invariant space vectors from
$\bar{A}$ and then recover invariant space vectors,$V$, for $A$.

\begin{algrthm}
\label{alg:invariantSpace}
\begin{program}
\mbox{Given $ \Gamma^{\sharp,\ast},Z^{\sharp,\ast},Z^{\flat,\ast} $,}
\mbox{compute vectors spanning the left invariant }
\mbox{space associated with large eigenvalues}
\FUNCT \mathcal{F}_{\ref{alg:invariantSpace}}( \Gamma^{\sharp,\ast},Z^{\sharp,\ast},Z^{\flat,\ast})
A:=\begin{bmatrix}\begin{matrix}  0&I\end{matrix}\\ \Gamma^\sharp\end{bmatrix}
\{\bar{A},\Pi,J_0\}=|stateSpaceReducer|( A,Z^{\sharp,\ast},Z^{\flat,\ast} )
\{\bar{V},M\}:=|leftInvariantSpaceVectors|(\bar{A})
V=|stateSpaceExpander|(\bar{V},M,\Pi,J_0)
\ENDFUNCT
\end{program}
\end{algrthm}

\subsubsection{Zero Invariant Space}


\begin{thrm}
The $Z^\sharp_\ast,Z^\flat_\ast$ span the invariant space 
associated with zero eigenvalue.
\begin{gather*}
  \begin{bmatrix}
Z^\sharp_\ast\\Z^\flat_\ast      
  \end{bmatrix}A^{L(\tau+\theta)} = 0
\end{gather*}
\end{thrm}
\begin{prf}
  \begin{gather*}
A^{\tau + \theta}=
  \begin{bmatrix}
    \Gamma_1\\ \Gamma_1^2 \\ \vdots \\ \Gamma_1^{\tau+\theta-1}
  \end{bmatrix}
  \begin{bmatrix}
    \Gamma_{\tau+\theta}&\Gamma_{1}
  \end{bmatrix}
  \end{gather*}
\end{prf}
Consider the Row Echelon form 
\begin{gather*}
Z= R^T_Z  \begin{bmatrix}
Z^\sharp_\ast\\Z^\flat_\ast
  \end{bmatrix}
\end{gather*}
The rank deficiency of this matrix is equal to the rank of the minimal
dimension transition matrix for calculating the eigenvalues and eigenvectors
of the full problem.







To construct the state space reduction,
extend $Z$ to a basis for the whole space.\cite[page 119]{NOBLE}

\begin{gather*}
  \begin{bmatrix}
    Z \\ \bar{Z}
  \end{bmatrix}\\
\intertext{ We have then}%\\
Z A = J_0 Z
\end{gather*}
 and 
$  \begin{bmatrix}
    Z \\ \bar{Z}
  \end{bmatrix}$ 
non-singular.

Consequently, after defining $Z_l, Z_r, \bar{Z}_l, \bar{Z}_r$ from 
\begin{gather*}
    \begin{bmatrix}
    Z \\ \bar{Z}
  \end{bmatrix}
^{-1} = 
  \begin{bmatrix}
    Z_l&Z_r\\
\bar{Z}_l&\bar{Z}_r
  \end{bmatrix}\\ \intertext{ we have }
  \begin{bmatrix}
    Z \\ \bar{Z}
  \end{bmatrix} A 
  \begin{bmatrix}
    Z_l&Z_r\\
\bar{Z}_l&\bar{Z}_r
  \end{bmatrix}\\
=
\begin{bmatrix}
  J_0&0\\
\Pi&\bar{A}
\end{bmatrix}\\
 \intertext{ with }
    \Pi=(\bar{Z}A 
\begin{bmatrix}
  Z_l\\ \bar{Z}_l
\end{bmatrix}
)\\
\bar{A}=
(\bar{Z}A   
  \begin{bmatrix}
Z_r\\ \bar{Z}_r
  \end{bmatrix})
\end{gather*}

\begin{thrm}
Suppose
\begin{gather*}
Y \bar{A} = M Y\\ \intertext{ so that $Y$ spans the invariant space associated with the eigenvalues of M, one can compute $X$ with }
\begin{bmatrix}
  X& Y
\end{bmatrix}\\ \intertext{spans the dominant invariant space of $A$. From }
vec(X)=((I \otimes M) - (J_0^T \otimes I))^{-1}(\Pi^T \otimes I) vec(Y)
\end{gather*}
\end{thrm}
\begin{prf}
  

Consequently,  if we compute
\begin{gather*}
Y \bar{A} = M Y\\ \intertext{ so that $Y$ spans the invariant space associated with the eigenvalues of M, one can compute $X$ with }
X J_0 + Y \Pi = MX\\ \intertext{ So that }
\begin{bmatrix}
  X& Y
\end{bmatrix}\\ \intertext{spans the dominant invariant space of $A$. Solving for $X$ we have  }
vec(X)=((I \otimes M) - (J_0^T \otimes I))^{-1}(\Pi^T \otimes I) vec(Y)\\
\intertext{ since }
vec(A B C) = (C^T \otimes A) vec(B)
\end{gather*}
\begin{gather*}
  V = 
  \begin{bmatrix}
    X&Y
  \end{bmatrix}
\end{gather*}
\end{prf}

\begin{algrthm}
\label{alg:zeroSquash}
\begin{program}
\mbox{Given $h,H$,}
\mbox{asymptotic stability constraints}
\FUNCT \mathcal{F}_{\ref{alg:zeroSquash}}(V,Z^{\sharp,\ast})
\ENDFUNCT
\end{program}
\end{algrthm}

 
 
  

\begin{exmpl}
  

\input{simsSquash.tex}

\end{exmpl}


\subsection{Implementation of State Space Transition Matrix Computation}

One can calculate these matrix transformation on a single 
 copy of the $H^{\sharp,k}$ 
saving on unnecessary  repetition of the coefficients 
present in the full tableau.
Furthermore, for many models,  leading rows of specific equations 
may be zero. One should immediately fill in initial rows of $Z^\sharp$ 
with these obvious shift right equations.


\subsubsection{Symbolic Algebra Implementation}


In symbolic algebra packages using exact arithmetic, there is
no need to worry about roundoff error and one can 
compute the row-echelon form of an augmented
$H^{\sharp,k}$ to determine rank and to 
obtain matrices for annihilating its rows\cite[page 119]{NOBLE}.
By augmenting the matrix with columns of the identity matrix, one
obtains a full rank matrix that either inverts $H^{\sharp,k}$ or zeroes
some of its rows.
\begin{gather*}
  rowEchelon(
  \begin{bmatrix}
    H^{\sharp,k}&I
  \end{bmatrix} = 
  \begin{bmatrix}
R    &U^{\sharp,k}
  \end{bmatrix}
)
\end{gather*}


\subsubsection{Numeric Implementation}


For numerical computations one should use the numerically stable unitary matrices generated by
QR Decomposition with column pivoting to annihilate rows.
Note that one can 
facilitate subsequent applications of the routines for QR Decomposition
 by putting the shifted rows corresponding to the annihilated rows
at the top of the  matrix.
Then, $H^{\sharp,k+1}$ will be closer to Hessenberg form. 
QR Decomposition works faster on 
Hessenberg matrices.
Indeed if only one row was annihilated, the new matrix will be 
in Hessenberg form.

With the final QR decomposition in hand,
it will not be necessary to invert
$H^{\sharp,\ast}_\theta= Q^{\sharp,\ast} R^{\sharp,\ast}$. Since 
$R^{\sharp,\ast}$ will be upper triangular and $(Q^{\sharp,\ast})^{-1}=(Q^{\sharp,\ast})^T$, we can back solve and multiply to get the columns of $\Gamma$.





%\input{simsTableaus.tex}

The construction of the companion matrix leads to a very sparse
matrix.
Note the sparsity of the transition matrix for the example problem in Equation \ref{AExample}.
Section \ref{sec:inessential}  shows how to 
 construct a reduced dimensions non-singular matrix for studying
the asymptotic dynamics of the linear system.




Floating point operation counts in Section \ref{sec:inessential} demonstrate
the utility of  computing these auxiliary constraints in both directions.
\begin{figure*}[htbp]
  \begin{center}
    \leavevmode
    
\fbox{
\begin{pspicture}(15,10)

\psframe[fillstyle=solid,fillcolor=gray](5,5)(10,7.5)%zapped
\psframe[fillstyle=solid,fillcolor=gray](5,5)(10,2.5)%zapped

\psline[linewidth=0.6mm](0,5)(15,5)%vertical left
\psline[linewidth=0.6mm](5,0)(5,10)%midline
\psline[linewidth=0.6mm](10,0)(10,10)%vertical right

\psframe[fillstyle=solid,fillcolor=gray](5,2.5)(11,2.0)%mid front
\psline(10,2.5)(11,2.5)
\psline(10,2.0)(11,2.0)
\psframe(5,2.5)(11,3.0)
\psframe[fillstyle=solid,fillcolor=gray](6,2.0)(12,1.0)
\psframe[linestyle=dashed,linewidth=0.7mm](5,1.5)(11,2.5)
\psframe[fillstyle=solid,fillcolor=black](10,1.5)(11,2.5)
\psframe[fillstyle=solid,fillcolor=white](11,1.5)(12,2.0)


\psframe[fillstyle=solid,fillcolor=gray](4,7.0)(10,8.0)%mid back
\psline(4,7.5)(4,7.0)
\psline(4,7.0)(5,7.0)
\psframe[fillstyle=solid,fillcolor=white](4,7.5)(5,7.0)
\psframe[fillstyle=solid,fillcolor=gray](3,8.0)(9,9.0)
\psframe[linestyle=dashed,linewidth=0.7mm](4,7.5)(10,8.5)
\psframe[fillstyle=solid,fillcolor=black](4,8.5)(5,7.5)
\psframe[fillstyle=solid,fillcolor=white](3,8.5)(4,8.0)

\psset{arrows=->}
\rput(10.5,2.5){\rnode{D}{}}
\rput(12.5,7.5){\rnode{E}{$H^{\sharp,k}$ with $H^{\sharp,k}_\theta$ non-singular}}
\nccurve[angleB=90,angleA=270]{E}{D}
\rput(5.0,4.0){\rnode{F}{}}
\rput(2.5,4.5){\rnode{G}{$Z^{\sharp,k}$}}
\nccurve[angleB=180]{G}{F}
\rput(5.0,2.75){\rnode{H}{}}
\rput(2.5,1.5){\rnode{G}{$F^{\sharp,k}$}}
\nccurve[angleB=180]{G}{H}

\rput(4.5,7.5){\rnode{D}{}}
\rput(1.5,7.5){\rnode{E}{$H^{\flat,k}$ with $H^{\flat,k}_\theta$ non-singular}}
\nccurve[angleB=270]{E}{D}
\rput(5.0,6.0){\rnode{F}{}}
\rput(1.5,5.5){\rnode{G}{$Z^{\flat,k}$}}
\nccurve[angleB=180]{G}{F}
\rput(5.0,7.25){\rnode{H}{}}
\rput(1.5,6.5){\rnode{G}{$F^{\flat,k}$}}
\nccurve[angleB=180]{G}{H}


\end{pspicture}
}

    \caption{Matrix Tableau Characterization of Algorithm: Full Rank Trailing Block and Leading Block }
    \label{fullRankBoth}
  \end{center}
\end{figure*}
Section \ref{sec:inessential} shows how to use  the vectors, 
that arise from computing the autoregressive representations,
$Z^{\sharp,\ast}, Z^{\flat,\ast}$, to construct a 
similarity transformation which makes it possible to compute the non zero 
eigenvalues and eigenvectors of 
the transition matrix with much smaller matrices.
This can be very important for large sparse models since, as Section
\ref{sec:invariantSpace} shows,  the computational
burden of the asymptotic analysis  increases with the cube of the problem
size.


\subsection{Implementation of the Invariant Space Computation}

Numerical routines for computing the invariant space order the eigenvalues
large to small and compute invariant space vectors for the dominant invariant
space (ie the invariant space associated with the largest eigenvalues).
Invariant space vectors spanning the invariant space are generally easier
to compute than eigenvectors.
In subsequent calculations, one can 
choose the invariant space calculation dimension
 to be one  more than the dimension of the required invariant
space. 
Although root counting alone cannot discover all model anomalies, often
a modeler knows enough about the problem to infer from the root count
whether the model seems appropriate for further analysis.
If this is the case, one can intervene after the Schur decomposition
of the transition matrix to 
abort further calculations.



\subsubsection{Symbolic Algebra Implementation}



All is not lost when 
symbolic algebra packages fail to compute the entire set of
eigenvectors in a reasonable amount of time.
Symbolic algebra packages can often quickly obtain expressions for the
eigenvalues of the matrices. Having identified which of these corresponds to
the large eigenvalues, one can compute the relevant
eigenvectors by computing vectors for
the null space of $(A^T-\lambda I)$.

\subsubsection{ Numeric Implementation}

The LAPACK routine DGEES computes the real-Schur form and the matrix of Schur
vectors forming an  orthonormal basis for the dominant invariant subspace of 
a real matrix A. Given an integer m, the algorithm computes a matrix V with
m orthonormal columns and a
real quasi triangular matrix T of order m such that 
\begin{gather*}
V A  =   T V
\end{gather*}

The eigenvalues of T are approximations to the m largest eigenvalues of A.
The columns of V span the invariant subspace corresponding 
to those eigenvalues of A.
Alternatively, the routine dnaupd in ARPACK applies Krylov methods for computing
the invariant space vectors.



\subsection{Implementation of the State Space Reduction}

 

For symbolic computation, one can compute the row-echelon form and use the
technique outlined in Noble to extend the rows to a basis. The most computationally intensive step will involve symbolic computation of the inverse.






For numerical calculations one can avoid a costly matrix inversion
by computing the QR Decomposition
\begin{gather*}
Q_Z R_Z=\begin{bmatrix}
(Z^{\sharp,\ast})^T  &(Z^{\flat,\ast})^T  
\end{bmatrix}
\end{gather*}
and to use the orthonormal columns of $Q^T$ for $Z$.
Compute
\begin{gather*}
  Z^T = Q_Z R_Z\\
\bar{\bar{Z}}^T = Q_Z Q_Z^T - I\\
\bar{\bar{Z}}^T=Q_{\bar{Z}} R_{\bar{Z}}\\
\bar{Z}=Q_{\bar{Z}}^T
\end{gather*}

Thus, the transpose provides the inverse of the matrix.

Now 
\begin{gather*}
    \begin{bmatrix}
    Z \\ \bar{Z}
  \end{bmatrix}
^{-1} =     \begin{bmatrix}
    Z \\ \bar{Z}
  \end{bmatrix}
^T= 
  \begin{bmatrix}
    Z_l&Z_r\\
\bar{Z}_l&\bar{Z}_r
  \end{bmatrix} 
\end{gather*}




\section{Numerical Experiments}

\subsection{Total Floating Point Operation Counts}
\label{sec:summaryFlops}


\input{biDirFLOPSSummary.tex}
\label{sec:exper}
\input{simsExampleN.tex}

The graph in Figure\ref{fig:simsFLOPS} demonstrates that the benefits from problem reduction dominate
the costs for all values of $N$ and for all values of $\frac{\lambda_k}{\lambda_{k-1}}$
Increasing $N$ increases the computational burden much more dramatically than 
changes in the rate of convergence.


\begin{figure}[htbp]
  \begin{center}
    \leavevmode
\epsfig{file=simsPlot.ps}    
    \caption{Reduction in Floating Point Operations as a Function of N}
    \label{fig:simsFLOPS}
  \end{center}
\end{figure}

\label{sec:numericalExp}


%\nocite{berry}
%\nocite{golub89}
%\nocite{taylor77,whiteman83}
%\nocite{krishnamurthy89}

\bibliography{files,anderson,paperstoget,aimUsers}

\appendix

% \section{Condition Numbers}
% \label{sec:conditionNo}

% \begin{gather*}
%   (H_- + \epsilon_1 F_1) + 
%   (H_0 + \epsilon_2 F_2)B(\epsilon_1,\epsilon_2,\epsilon_3) + 
%   (H_+ + \epsilon_3 F_3)B(\epsilon_1,\epsilon_2,\epsilon_3)^2 = 0\\
% \parallel F_i \parallel_2 =1\\
% \frac{\partial}{\partial \epsilon_i}
% \end{gather*}


\setcounter{thrm}{0}
\setcounter{crrlry}{0}



\section{Floating Point Operation Counts}
\label{sec:flops}


\subsection{General Floating Point Operation Counts}
\label{sec:genlFlops}

\input{biDirGolub.tex}

\subsection{Unconstrained Autoregression Floating Point Operation Counts}
\label{sec:unconstrainedFlops}


\input{biDirARCount.tex}


\subsection{Eigenspace Calculation Floating Point Operation Counts}
\label{sec:esFlops}

\input{biDirESCount.tex}

\subsection{Space Reduction Floating Point Operation Counts}
\label{sec:redFlops}


\input{biDirRedCount.tex}





\section{Proofs}
\label{sec:proofs}


\begin{thrm}
If $\mathcal{H}$ is full rank, then there exists a sequence of elementary row
operations that transforms $\mathcal{H}$ into $\mathcal{H}^\ast$
\begin{multline*}
\mathcal{H}^\ast= S \mathcal{H} =\\
\begin{bmatrix}
\zsh\\
\hsh\\
&&\ddots\\
&&\hsh\\  
&&&\hsht\\
\end{bmatrix}
\end{multline*}
 with non-singular $H^{\sharp\ast}_{\theta}$.
\end{thrm}
\begin{prf}
  Under construction.
\end{prf}
%\listinginput{1}{cleanAim.m}
%\end{latexonly}
}\end{document}
